{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape - X: (2186, 60, 4), y: (2186, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define stock and time range\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n",
    "    \n",
    "    # Reset index for proper datetime handling\n",
    "df.reset_index(inplace=True)\n",
    "    \n",
    "    # Ensure 'Adj Close' is a 1D Series\n",
    "adj_close = df['Adj Close'].squeeze()\n",
    "    \n",
    "    # Compute percentage return\n",
    "df['Return'] = adj_close.pct_change()\n",
    "\n",
    "    # Compute Technical Indicators\n",
    "df['RSI'] = ta.momentum.RSIIndicator(adj_close).rsi()\n",
    "df['EMA'] = ta.trend.EMAIndicator(adj_close).ema_indicator()\n",
    "df['ATR'] = ta.volatility.AverageTrueRange(\n",
    "        high=df['High'].squeeze(), \n",
    "        low=df['Low'].squeeze(), \n",
    "        close=adj_close\n",
    "    ).average_true_range()\n",
    "    \n",
    "df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(\n",
    "        high=df['High'].squeeze(), \n",
    "        low=df['Low'].squeeze(), \n",
    "        close=adj_close, \n",
    "        volume=df['Volume'].squeeze()\n",
    "    ).volume_weighted_average_price()\n",
    "\n",
    "    # Drop NaN values that appear due to indicator calculations\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "# scaler = MinMaxScaler()\n",
    "# df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "features = ['Adj Close', 'Volume', 'RSI', 'EMA']\n",
    "# lookback =14  # Total number of days window choosen\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df[features])\n",
    "\n",
    "# Define lookback period (past days for input)\n",
    "lookback = 60  # Example: use past 60 days to predict next 5 days\n",
    "n_steps_out = 5\n",
    "\n",
    "# Convert data into sequences for model training\n",
    "def create_sequences(data, lookback, n_steps_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - n_steps_out):\n",
    "        X.append(data[i : i + lookback])\n",
    "        y.append(data[i + lookback : i + lookback + n_steps_out,3])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create training sequences\n",
    "X, y = create_sequences(df_scaled, lookback, n_steps_out)\n",
    "feature_size = X.shape[2]\n",
    "\n",
    "print(f\"Dataset Shape - X: {X.shape}, y: {y.shape}\")  # Should be (samples, lookback, features), (samples, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Date  Adj Close      Close       High        Low       Open  \\\n",
      "Ticker                  AAPL       AAPL       AAPL       AAPL       AAPL   \n",
      "13     2015-01-22  25.003347  28.100000  28.117500  27.430000  27.565001   \n",
      "14     2015-01-23  25.132374  28.245001  28.437500  27.882500  28.075001   \n",
      "15     2015-01-26  25.159063  28.275000  28.590000  28.200001  28.434999   \n",
      "16     2015-01-27  24.278170  27.285000  28.120001  27.257500  28.105000   \n",
      "17     2015-01-28  25.650681  28.827499  29.530001  28.827499  29.407499   \n",
      "\n",
      "Price      Volume    Return        RSI        EMA       ATR       VWAP  \n",
      "Ticker       AAPL                                                       \n",
      "13      215185600  0.026016  60.986494  24.313966  3.246368  26.227638  \n",
      "14      185859200  0.005160  62.490274  24.423087  3.259781  26.274639  \n",
      "15      222460000  0.001062  62.809609  24.521218  3.273913  26.395273  \n",
      "16      382274800 -0.035013  48.218610  24.488811  3.251557  26.482635  \n",
      "17      585908400  0.056533  62.741698  24.643727  3.394434  26.751990  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubh\\.conda\\envs\\stock\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\shubh\\.conda\\envs\\stock\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, LeakyReLU, Dropout, Conv1D, Flatten, Bidirectional, Reshape\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "def Generator(n_steps_in, n_steps_out, feature_size, weight_initializer) -> tf.keras.models.Model:\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=2, strides=1, \n",
    "                     padding='same', kernel_initializer=weight_initializer, \n",
    "                     input_shape=(n_steps_in, feature_size)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Bidirectional(LSTM(64, activation='relu', kernel_initializer=weight_initializer, \n",
    "                                 return_sequences=False, dropout=0.3)))\n",
    "    \n",
    "    model.add(Dense(64, activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(n_steps_out, activation='linear'))  # Predicting 5 days ahead\n",
    "    model.add(Reshape((n_steps_out, 1)))  # Ensure output shape is (5, 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def Discriminator(weight_initializer, n_steps_in, n_steps_out, feature_size) -> tf.keras.models.Model:\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=2, strides=1, padding='same', \n",
    "                     kernel_initializer=weight_initializer, \n",
    "                     input_shape=(n_steps_in + n_steps_out, feature_size)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Conv1D(64, kernel_size=2, strides=1, kernel_initializer=weight_initializer, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64, activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create Generator & Discriminator\n",
    "weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "generator = Generator(lookback, n_steps_out, feature_size, weight_initializer)\n",
    "discriminator = Discriminator(weight_initializer, lookback, n_steps_out, feature_size)\n",
    "\n",
    "# WGAN Model Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(self, generator, discriminator, n_steps_in, n_steps_out):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(0.0004, beta_1=0.5, beta_2=0.9)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.n_steps_in = n_steps_in\n",
    "        self.n_steps_out = n_steps_out\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_output, generated_output):\n",
    "        \"\"\" Calculates the gradient penalty.\"\"\"\n",
    "        alpha = tf.random.normal([batch_size, self.n_steps_in + self.n_steps_out, feature_size], 0.0, 1.0)\n",
    "        diff = generated_output - tf.cast(real_output, tf.float32)\n",
    "        interpolated = tf.cast(real_output, tf.float32) + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected multiples argument to be a vector of length 4 but got length 3 [Op:Tile]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# **Fix the shape issue**\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Expand real_y and fake_y to match time steps (repeat across 60 time steps)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m real_y_expanded \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtile(real_y[:, tf\u001b[38;5;241m.\u001b[39mnewaxis, :], [\u001b[38;5;241m1\u001b[39m, real_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# (batch, 60, 5)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m fake_y_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, 60, 5)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Concatenate across the feature axis (last axis)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m real_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([real_x, real_y_expanded], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch, 60, 9)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.conda\\envs\\stock\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:15275\u001b[0m, in \u001b[0;36mtile\u001b[1;34m(input, multiples, name)\u001b[0m\n\u001b[0;32m  15273\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m  15274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m> 15275\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtile_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  15276\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  15277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m  15278\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.conda\\envs\\stock\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:15321\u001b[0m, in \u001b[0;36mtile_eager_fallback\u001b[1;34m(input, multiples, name, ctx)\u001b[0m\n\u001b[0;32m  15319\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, multiples]\n\u001b[0;32m  15320\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTmultiples\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Tmultiples)\n\u001b[1;32m> 15321\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  15322\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  15323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m  15324\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m  15325\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTile\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.conda\\envs\\stock\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected multiples argument to be a vector of length 4 but got length 3 [Op:Tile]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for real_x, real_y in dataset:\n",
    "        # Convert to float32\n",
    "        real_x = tf.cast(real_x, tf.float32)  # Shape: (batch, 60, 4)\n",
    "        real_y = tf.cast(real_y, tf.float32)  # Shape: (batch, 5)\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            fake_y = generator(real_x, training=True)\n",
    "            fake_y = tf.cast(fake_y, tf.float32)  # Shape: (batch, 5)\n",
    "\n",
    "            # **Fix the shape issue**\n",
    "            # Expand real_y and fake_y to match time steps (repeat across 60 time steps)\n",
    "            real_y_expanded = tf.tile(real_y[:, tf.newaxis, :], [1, real_x.shape[1], 1])  # (batch, 60, 5)\n",
    "            fake_y_expanded = tf.tile(fake_y[:, tf.newaxis, :], [1, real_x.shape[1], 1])  # (batch, 60, 5)\n",
    "\n",
    "            # Concatenate across the feature axis (last axis)\n",
    "            real_input = tf.concat([real_x, real_y_expanded], axis=-1)  # Shape: (batch, 60, 9)\n",
    "            fake_input = tf.concat([real_x, fake_y_expanded], axis=-1)  # Shape: (batch, 60, 9)\n",
    "\n",
    "            real_output = discriminator(real_input, training=True)\n",
    "            fake_output = discriminator(fake_input, training=True)\n",
    "\n",
    "            d_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "            gp = wgan.gradient_penalty(batch_size, real_output, fake_output)\n",
    "            d_loss += 10 * gp  # Apply gradient penalty\n",
    "\n",
    "        grads = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        wgan.d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "        # Train generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake_y = generator(real_x, training=True)\n",
    "            fake_y = tf.cast(fake_y, tf.float32)  # Shape: (batch, 5)\n",
    "\n",
    "            # Expand fake_y to match time steps\n",
    "            fake_y_expanded = tf.tile(fake_y[:, tf.newaxis, :], [1, real_x.shape[1], 1])  # (batch, 60, 5)\n",
    "\n",
    "            fake_input = tf.concat([real_x, fake_y_expanded], axis=-1)  # Shape: (batch, 60, 9)\n",
    "            fake_output = discriminator(fake_input, training=True)\n",
    "            g_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "        grads = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        wgan.g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "    print(f\"Epoch {epoch} - D Loss: {d_loss.numpy()}, G Loss: {g_loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the last known 60 days of data as input\n",
    "latest_data = df_scaled.iloc[-lookback:].values.reshape(1, lookback, feature_size)\n",
    "\n",
    "# Predict next 5 days\n",
    "predicted_future = generator.predict(latest_data)\n",
    "\n",
    "# Convert predictions back to actual values\n",
    "predicted_future_prices = scaler.inverse_transform(\n",
    "    np.hstack((np.zeros((5, feature_size - 1)), predicted_future.reshape(-1, 1)))\n",
    ")[:, -1]\n",
    "\n",
    "print(\"Predicted Prices for Next 5 Days:\", predicted_future_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
